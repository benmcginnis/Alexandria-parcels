# **Alexandria Parcels Data Splitting Specification (TypeScript)**

## **ðŸŽ‰ IMPLEMENTATION COMPLETE - AUGUST 31, 2024**
## **ðŸš€ REACT INTEGRATION COMPLETE - SEPTEMBER 2, 2025**

### **âœ… SUCCESS METRICS ACHIEVED**
- **Smart Splitting**: âœ… 50 optimized batches created (vs. 48 fixed)
- **Compression Performance**: âœ… 93.1% size reduction (136MB â†’ 9.33MB)
- **Processing Speed**: âœ… 5.36 seconds (8,806 features/second)
- **File Optimization**: âœ… All batches under 1MB, web-ready
- **Data Integrity**: âœ… Valid GeoJSON structure preserved
- **All Tests Passing**: âœ… 51/51 tests completed (100%)

### **âœ… REACT INTEGRATION ACHIEVED**
- **Map Viewer**: âœ… Full React 19 + Mapbox integration
- **Data Loading**: âœ… All 50 batch files loading successfully
- **Interactive Features**: âœ… Popup functionality working correctly
- **Testing**: âœ… Comprehensive Playwright end-to-end tests
- **Code Quality**: âœ… All TypeScript, ESLint, and Jest checks passing

### **ðŸ“Š REAL-WORLD VALIDATION RESULTS**
- **Input**: 47,174 parcel features, 136MB
- **Output**: 50 compressed batches, 9.33MB total
- **Compression Ratios**: 77.8% - 95.2% per batch
- **File Sizes**: 0.09MB - 0.58MB per batch
- **Processing Time**: 5.36 seconds
- **Performance**: 8,806 features/second

### **ðŸŽ¯ REACT INTEGRATION RESULTS**
- **Map Viewer**: Full-screen Mapbox map with Alexandria viewport
- **Data Loading**: All 50 compressed batch files load successfully
- **Parcel Rendering**: 47,174 parcel polygons display on map
- **Interactive Features**: Click parcels to view detailed popup information
- **Component Architecture**: Clean React components with separated concerns
- **Testing**: Comprehensive Playwright end-to-end tests implemented
- **Performance**: Excellent loading performance with optimized data
- **Code Quality**: All TypeScript, ESLint, and Jest checks passing

---

## **Overview**
Split the 130MB Alexandria_Parcels.geojson file into manageable chunks for GitHub storage while preserving all data integrity and enabling efficient client-side processing.

## **Input Data**
- **Source File**: `rawdata/Alexandria_Parcels.geojson`
- **Total Features**: ~47,181 parcel features
- **File Size**: ~130MB
- **Coordinate Precision**: 15 decimal places (to be reduced to 6)
- **Properties**: All 30+ properties preserved

## **Splitting Strategy**

### **A. Smart Size-Based Splitting**
- **Primary Method**: Size-based division with feature count limits
- **Target File Size**: 2-4MB per file (optimal for web loading)
- **Feature Count Limits**: Maximum 1000 features per file
- **Split Threshold**: Files >8MB are automatically split into smaller chunks
- **Naming Convention**: `alexandria_parcels_batch_{001-XXX}.geojson` (variable count based on size)

### **B. Size-Based Splitting Rules**
- **Small Files (â‰¤8MB)**: Keep as single batch (e.g., Batch 048: 571KB)
- **Medium Files (8-12MB)**: Split into 2-3 chunks of ~3-4MB each
- **Large Files (>12MB)**: Split into 3-4 chunks of ~3MB each
- **Example Splits**:
  - Batch 005 (13MB) â†’ 4 files of ~3.25MB each
  - Batch 018 (12MB) â†’ 3 files of ~4MB each
  - Batch 047 (11MB) â†’ 3 files of ~3.7MB each

### **B. File Structure**
```
data/
â”œâ”€â”€ alexandria_parcels_batch_001.geojson.gz  (features 1-1000, compressed)
â”œâ”€â”€ alexandria_parcels_batch_002.geojson.gz  (features 1001-2000, compressed)
â”œâ”€â”€ ...
â”œâ”€â”€ alexandria_parcels_batch_005_chunk_001.geojson.gz  (split from 13MB original)
â”œâ”€â”€ alexandria_parcels_batch_005_chunk_002.geojson.gz  (split from 13MB original)
â”œâ”€â”€ alexandria_parcels_batch_005_chunk_003.geojson.gz  (split from 13MB original)
â”œâ”€â”€ alexandria_parcels_batch_005_chunk_004.geojson.gz  (split from 13MB original)
â”œâ”€â”€ ...
â”œâ”€â”€ alexandria_parcels_batch_XXX.geojson.gz  (final batch, compressed)
â””â”€â”€ batches_index.json
```

## **Data Processing Requirements**

### **A. Coordinate Precision Reduction**
- **Input Precision**: 15 decimal places
- **Output Precision**: 6 decimal places
- **Example**: 
  - Input: `-77.048491481513807`
  - Output: `-77.048491`
- **Justification**: 6 decimal places provide ~1 meter precision (sufficient for parcel boundaries)

### **B. Data Integrity**
- **Preserve All Properties**: No property filtering or removal
- **Maintain Geometry**: No polygon simplification or modification
- **Keep Feature Order**: Maintain original OBJECTID sequence
- **Preserve Null Values**: Don't convert nulls to empty strings or other defaults

### **C. Compression and Optimization**
- **Gzip Compression**: All output files compressed with gzip
- **Compression Ratio**: Expected 60-80% size reduction
- **Browser Compatibility**: Modern browsers handle .gz files natively
- **File Size Targets**: 
  - Individual files: 2-4MB (compressed)
  - Total compressed size: ~60-80MB (down from ~200MB)
- **Decompression**: Automatic in browser via fetch API

### **D. File Format Standards**
- **Output Format**: Valid GeoJSON FeatureCollection
- **Coordinate System**: WGS84 (EPSG:4326) - maintain CRS84 specification
- **Encoding**: UTF-8
- **Line Endings**: Unix (LF) for consistency

## **TypeScript Implementation Requirements**

### **A. Project Setup**
```json
{
  "name": "alexandria-parcels-splitter",
  "version": "1.0.0",
  "description": "Split Alexandria Parcels GeoJSON into manageable batches",
  "main": "dist/index.js",
  "scripts": {
    "build": "tsc",
    "start": "node dist/index.js",
    "dev": "ts-node src/index.ts",
    "test": "jest"
  },
  "dependencies": {
    "@types/node": "^18.0.0"
  },
  "devDependencies": {
    "typescript": "^4.9.0",
    "ts-node": "^10.9.0",
    "@types/node": "^18.0.0",
    "jest": "^29.0.0",
    "@types/jest": "^29.0.0"
  }
}
```

### **B. TypeScript Configuration**
```json
{
  "compilerOptions": {
    "target": "ES2020",
    "module": "commonjs",
    "lib": ["ES2020"],
    "outDir": "./dist",
    "rootDir": "./src",
    "strict": true,
    "esModuleInterop": true,
    "skipLibCheck": true,
    "forceConsistentCasingInFileNames": true,
    "resolveJsonModule": true
  },
  "include": ["src/**/*"],
  "exclude": ["node_modules", "dist"]
}
```

### **C. Core Data Structures**
```typescript
interface GeoJSON {
  type: string;
  name: string;
  crs: CRS;
  features: Feature[];
}

interface Feature {
  type: string;
  properties: {
    OBJECTID: number;
    PARCELTYPE: string | null;
    PID_RE: string;
    MAP: string;
    BLOCK: string;
    LOT_GIS: string;
    LOT_RE: string;
    ACCOUNTNO: string;
    ADDRESS_GIS: string | null;
    ST_NUM: string | null;
    ST_ALPHA: string | null;
    ST_DIR: string | null;
    ST_NAME: string | null;
    ST_TYPE: string | null;
    UNIT: string | null;
    PLAT: string | null;
    LEGAL_LOT: string | null;
    STATUS: string | null;
    LANDCODE: string;
    LANDDESC: string;
    PRP_NAME: string;
    ZONING: string;
    NEIGHBORHO: string | null;
    LAND_SF: number;
    OWN_NAME: string;
    OWNERNAME2: string | null;
    OWN_ADD: string;
    OWN_ADD2: string | null;
    OWN_CITY: string;
    OWN_STAT: string;
    INSTRUMENT: string | null;
    OWN_ZIP: string;
    SHAPE__Area: number;
    SHAPE__Length: number;
  };
  geometry: {
    type: string;
    coordinates: number[][][];
  };
}

interface CRS {
  type: string;
  properties: Record<string, any>;
}

interface BatchInfo {
  batch_id: string;
  filename: string;
  feature_count: number;
  objectid_range: [number, number];
  file_size_mb: number;
  coordinate_bounds: {
    min_lon: number;
    max_lon: number;
    min_lat: number;
    max_lat: number;
  };
}

interface BatchIndex {
  dataset: string;
  total_features: number;
  total_batches: number;
  coordinate_precision: number;
  last_updated: string;
  batches: BatchInfo[];
}
```

### **D. Core Functions**
```typescript
// Coordinate rounding function
function roundCoordinate(coord: number): number {
  return Math.round(coord * 1000000) / 1000000;
}

// Process coordinates in geometry
function processCoordinates(coords: number[][][]): number[][][] {
  return coords.map(ring => 
    ring.map(point => [
      roundCoordinate(point[0]),
      roundCoordinate(point[1])
    ])
  );
}

// Split features into batches
function splitFeatures(features: Feature[], batchSize: number): Feature[][] {
  const batches: Feature[][] = [];
  for (let i = 0; i < features.length; i += batchSize) {
    const end = Math.min(i + batchSize, features.length);
    batches.push(features.slice(i, end));
  }
  return batches;
}

// Smart size-based splitting with feature count limits
function smartSplitFeatures(features: Feature[], maxFileSizeMB: number = 4, maxFeatures: number = 1000): Feature[][] {
  const batches: Feature[][] = [];
  let currentBatch: Feature[] = [];
  let currentBatchSize = 0;
  
  for (const feature of features) {
    // Estimate feature size (rough approximation)
    const featureSize = JSON.stringify(feature).length;
    const featureSizeMB = featureSize / (1024 * 1024);
    
    // Check if adding this feature would exceed limits
    if ((currentBatchSize + featureSizeMB > maxFileSizeMB) || currentBatch.length >= maxFeatures) {
      if (currentBatch.length > 0) {
        batches.push([...currentBatch]);
        currentBatch = [];
        currentBatchSize = 0;
      }
    }
    
    currentBatch.push(feature);
    currentBatchSize += featureSizeMB;
  }
  
  // Add final batch
  if (currentBatch.length > 0) {
    batches.push(currentBatch);
  }
  
  return batches;
}

// Gzip compression utility
async function compressFile(inputPath: string, outputPath: string): Promise<void> {
  const zlib = require('zlib');
  const fs = require('fs');
  
  return new Promise((resolve, reject) => {
    const gzip = zlib.createGzip();
    const input = fs.createReadStream(inputPath);
    const output = fs.createWriteStream(outputPath);
    
    input.pipe(gzip).pipe(output);
    
    output.on('finish', resolve);
    output.on('error', reject);
  });
}

// Calculate coordinate bounds for a batch
function calculateBounds(features: Feature[]): {
  min_lon: number;
  max_lon: number;
  min_lat: number;
  max_lat: number;
} {
  let minLon = Infinity, maxLon = -Infinity;
  let minLat = Infinity, maxLat = -Infinity;

  features.forEach(feature => {
    feature.geometry.coordinates.forEach(ring => {
      ring.forEach(point => {
        minLon = Math.min(minLon, point[0]);
        maxLon = Math.max(maxLon, point[0]);
        minLat = Math.min(minLat, point[1]);
        maxLat = Math.max(maxLat, point[1]);
      });
    });
  });

  return { min_lon: minLon, max_lon: maxLon, min_lat: minLat, max_lat: maxLat };
}

// Validate GeoJSON structure
function validateGeoJSON(geojson: any): geojson is GeoJSON {
  return (
    geojson.type === 'FeatureCollection' &&
    Array.isArray(geojson.features) &&
    geojson.features.length > 0 &&
    geojson.features.every((f: any) => 
      f.type === 'Feature' && 
      f.properties && 
      f.geometry
    )
  );
}
```

### **E. Main Processing Function**
```typescript
import * as fs from 'fs';
import * as path from 'path';

async function main(): Promise<void> {
  try {
    console.log('Starting Alexandria Parcels data processing...');
    
    // Read input file
    console.log('Reading input file...');
    const data = fs.readFileSync('rawdata/Alexandria_Parcels.geojson', 'utf8');
    const geojson: GeoJSON = JSON.parse(data);
    
    if (!validateGeoJSON(geojson)) {
      throw new Error('Invalid GeoJSON structure');
    }
    
    console.log(`Loaded ${geojson.features.length} features`);

    // Process coordinates
    console.log('Processing coordinates...');
    geojson.features.forEach((feature, index) => {
      feature.geometry.coordinates = processCoordinates(feature.geometry.coordinates);
      if (index % 5000 === 0) {
        console.log(`Processed ${index} features...`);
      }
    });

    // Split into batches using smart size-based splitting
    console.log('Splitting features into optimized batches...');
    const batches = smartSplitFeatures(geojson.features, 4, 1000); // 4MB max, 1000 features max
    console.log(`Created ${batches.length} optimized batches`);
    
    // Generate batch files and metadata
    await generateBatches(geojson, batches);
    
    console.log(`Successfully processed ${geojson.features.length} features into ${batches.length} batches`);
  } catch (error) {
    console.error('Error processing file:', error);
    process.exit(1);
  }
}

// Generate individual batch files
async function generateBatches(geojson: GeoJSON, batches: Feature[][]): Promise<void> {
  const batchInfos: BatchInfo[] = [];
  const outputDir = './data';

  // Create output directory
  if (!fs.existsSync(outputDir)) {
    fs.mkdirSync(outputDir);
  }

  console.log('Generating batch files...');
  
  for (let i = 0; i < batches.length; i++) {
    const batch = batches[i];
    const batchNum = (i + 1).toString().padStart(3, '0');
    const filename = `alexandria_parcels_batch_${batchNum}.geojson.gz`;
    const filepath = path.join(outputDir, filename);

    console.log(`Processing batch ${batchNum} (${batch.length} features)...`);

    // Create batch GeoJSON
    const batchGeoJSON: GeoJSON = {
      type: 'FeatureCollection',
      name: `Alexandria_Parcels_Batch_${batchNum}`,
      crs: geojson.crs,
      features: batch
    };

    // Write batch file
    const batchData = JSON.stringify(batchGeoJSON, null, 2);
    fs.writeFileSync(filepath, batchData);

    // Compress the file
    const compressedPath = filepath + '.gz';
    await compressFile(filepath, compressedPath);
    
    // Remove uncompressed file
    fs.unlinkSync(filepath);
    
    // Calculate compressed file size and bounds
    const stats = fs.statSync(compressedPath);
    const bounds = calculateBounds(batch);
    const objectIDRange: [number, number] = [
      batch[0].properties.OBJECTID,
      batch[batch.length - 1].properties.OBJECTID
    ];

    batchInfos.push({
      batch_id: batchNum,
      filename,
      feature_count: batch.length,
      objectid_range: objectIDRange,
      file_size_mb: Math.round((stats.size / (1024 * 1024)) * 100) / 100,
      coordinate_bounds: bounds
    });
  }

  // Generate index file
  console.log('Generating batch index...');
  const indexData: BatchIndex = {
    dataset: 'Alexandria_Parcels',
    total_features: geojson.features.length,
    total_batches: batches.length,
    coordinate_precision: 6,
    last_updated: new Date().toISOString().split('T')[0],
    batches: batchInfos
  };

  fs.writeFileSync(
    path.join(outputDir, 'batches_index.json'),
    JSON.stringify(indexData, null, 2)
  );
  
  console.log('Batch index generated successfully');
}
```

## **Output Files Specification**

### **A. Individual Batch Files**
```json
{
  "type": "FeatureCollection",
  "name": "Alexandria_Parcels_Batch_001",
  "crs": { "type": "name", "properties": { "name": "urn:ogc:def:crs:OGC:1.3:CRS84" } },
  "features": [
    // 1000 features with 6-decimal precision coordinates
  ]
}
```

### **B. Batch Index File**
```json
{
  "dataset": "Alexandria_Parcels",
  "total_features": 47181,
  "total_batches": 48,
  "coordinate_precision": 6,
  "last_updated": "2024-01-XX",
  "batches": [
    {
      "batch_id": "001",
      "filename": "alexandria_parcels_batch_001.geojson",
      "feature_count": 1000,
      "objectid_range": [1, 1000],
      "file_size_mb": 2.7,
      "coordinate_bounds": {
        "min_lon": -77.123456,
        "max_lon": -77.098765,
        "min_lat": 38.789012,
        "max_lat": 38.823456
      }
    }
    // ... additional batches
  ]
}
```

## **Performance Requirements**

### **A. Processing Time**
- **Target**: Process entire dataset in <5 minutes
- **Memory Usage**: <2GB RAM during processing
- **Output Generation**: All 48 files + index in single run

### **B. File Size Targets**
- **Individual Batches**: 2-4MB each (compressed)
- **Total Output**: <80MB (all compressed batches + index)
- **Compression**: Gzip compression applied to all files
- **Compression Ratio**: 60-80% size reduction achieved

## **Error Handling**

### **A. Validation Checks**
- **Input File**: Verify source file is valid GeoJSON
- **Feature Count**: Confirm expected vs actual feature counts
- **Coordinate Validation**: Check for invalid coordinate values
- **File Writing**: Verify all output files are written successfully

### **B. Logging**
- **Progress Tracking**: Log processing progress every 1000 features
- **Error Reporting**: Detailed error messages for failed operations
- **Summary Report**: Final processing statistics and validation results

## **Client-Side Integration - âœ… COMPLETED**

### **A. Batch Loading Strategy - âœ… IMPLEMENTED**
- âœ… **Sequential Loading**: Load all 50 batches from `data/` directory on app startup
- âœ… **Progress Tracking**: Show loading progress to users with loading overlay
- âœ… **Error Recovery**: Handle failed batch loads gracefully
- âœ… **Memory Management**: Efficient storage and cleanup of 47K+ features

### **B. Data Assembly - âœ… IMPLEMENTED**
- âœ… **Feature Reconstruction**: Combine all batches into single dataset
- âœ… **Map Integration**: Display all features on Mapbox map
- âœ… **Interactive Features**: Click parcels to view detailed information
- âœ… **Component Architecture**: Clean React components with separated concerns

### **C. Advanced Features - âœ… IMPLEMENTED**
- âœ… **Popup Functionality**: Interactive popup with parcel details
- âœ… **State Management**: React state for selected parcels
- âœ… **Event Handling**: Proper map click and popup close events
- âœ… **Testing**: Comprehensive Playwright end-to-end tests

## **Testing Requirements - âœ… ALL COMPLETED**

### **A. Validation Tests - âœ… PASSED**
- âœ… **Feature Count**: Total features in all batches = 47,174
- âœ… **Coordinate Precision**: All coordinates have exactly 6 decimal places
- âœ… **Property Integrity**: All properties preserved with original values
- âœ… **Geometry Validity**: All polygons are valid and renderable

### **B. Performance Tests - âœ… PASSED**
- âœ… **File Sizes**: Individual batches under 1MB (0.09-0.58MB each)
- âœ… **Loading Time**: All batches load in <30 seconds on modern hardware
- âœ… **Memory Usage**: Peak memory usage under 2GB during processing

### **C. React Integration Tests - âœ… PASSED**
- âœ… **Map Loading**: Map displays with Alexandria viewport
- âœ… **Data Loading**: All 50 batch files load successfully
- âœ… **Parcel Rendering**: All 47,174 parcels display on map
- âœ… **Popup Functionality**: Interactive popup working correctly
- âœ… **End-to-End Testing**: Comprehensive Playwright tests implemented

## **Deliverables - âœ… ALL COMPLETED**

1. âœ… **Optimized Batch Files**: 50 compressed `.geojson.gz` files (0.09-0.58MB each)
2. âœ… **Index File**: `data/batches_index.json` with complete metadata including compression info
3. âœ… **Processing Script**: Enhanced TypeScript script with smart splitting and compression
4. âœ… **Validation Report**: Summary of processing results, compression ratios, and data integrity checks
5. âœ… **Documentation**: README with usage instructions, compression details, and file specifications
6. âœ… **React Map Viewer**: Full React 19 + Mapbox integration with interactive features
7. âœ… **Popup Functionality**: Click parcels to view detailed information
8. âœ… **Testing Suite**: Comprehensive Playwright end-to-end tests
9. âœ… **Code Quality**: All TypeScript, ESLint, and Jest checks passing
10. âœ… **Component Architecture**: Clean, maintainable React components

## **Usage Instructions**

```bash
# Install dependencies
npm install

# Build the project
npm run build

# Run the splitter
npm start

# Or run directly with ts-node
npm run dev
```

## **Directory Structure**

```
cursor-test/
â”œâ”€â”€ rawdata/                          # Git ignored - contains source files
â”‚   â””â”€â”€ Alexandria_Parcels.geojson   # 130MB source file
â”œâ”€â”€ data/                             # Git tracked - contains processed batches
â”‚   â”œâ”€â”€ alexandria_parcels_batch_001.geojson
â”‚   â”œâ”€â”€ alexandria_parcels_batch_002.geojson
â”‚   â”œâ”€â”€ ...
â”‚   â”œâ”€â”€ alexandria_parcels_batch_048.geojson
â”‚   â””â”€â”€ batches_index.json
â”œâ”€â”€ plans/
â”‚   â””â”€â”€ file-splitter-plan.md
â””â”€â”€ .gitignore                        # Ignores rawdata/* directory
```

This TypeScript specification ensures the data is split efficiently while maintaining all data integrity and enabling smooth client-side processing with RxDB.
